\section{Experiment Methodology}
This section details the experimental methodology used for a rigorous and reproducible comparison of the algorithms, focusing on performance in resource-constrained contexts. We follow established benchmarking principles \cite{resource_constrained, embedded_benchmarking}, emphasizing reproducibility and statistical validity.

\subsection{Goals and Metrics}
The primary goals are to compare algorithm performance and scalability, identify bottlenecks, and analyze resource utilization relevant to constrained environments. Key metrics collected include wall-clock execution time for core operations, peak memory allocation during execution, and input parameters such as bit length and algorithm-specific settings (e.g., Miller-Rabin rounds).

\subsection{Experimental Setup}
Baseline tests were conducted on a standard desktop system (Intel Core i7-9700K @ 3.60GHz, 32GB RAM) to establish reference performance. The software environment consisted of Ubuntu 22.04 LTS, GCC 11.4.0 compiling with the C++11 standard, and GMP 6.2.1. Timing and memory tracking were performed using a custom C++ application leveraging "high\_resolution\_clock".

\subsection{Procedure}
For each algorithm and input size configuration:
\begin{enumerate}
    \item Initialize data structures.
    \item Execute the core operation multiple times (>= 30 runs) for statistical significance, monitoring time and resources.
    \item Record raw performance metrics (time, memory).
    \item Calculate statistical summaries (average, stddev, min, max).
    \item Store all data systematically.
\end{enumerate}

\subsection{Data Analysis}
Data analysis involves statistical characterization of performance using mean, median, and standard deviation. Performance trends and scaling behavior are identified through visualization (e.g., plots of execution time vs. bit length) and direct comparison across algorithms based on the collected metrics.

\subsection{Reproducibility}
Reproducibility is ensured by making all source code for algorithms and benchmarks available in the repository, along with scripts for running benchmarks and parsing results. Experimental setup details (software versions, hardware) are documented, and raw data is archived.

\subsection{Limitations}
Several limitations should be noted. Benchmarks were conducted on a standard desktop, meaning performance may differ on actual resource-constrained hardware. Memory usage measurements are potentially approximate. The results reflect general C++/GMP performance without hardware-specific tuning. Finally, external factors like OS activity and cache state can introduce noise, although this is mitigated through multiple runs and statistical analysis.

This methodology adapts established benchmarking practices \cite{resource_constrained, embedded_benchmarking} for evaluating cryptographic primitives in resource-aware contexts. 