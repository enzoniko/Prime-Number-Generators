\section{Primality Testing and Generation}

This section covers the primality testing algorithms implemented in the project. As required by the assignment, we have implemented the Miller-Rabin primality test and, as our second choice, the Baillie-PSW primality test. These algorithms allow us to determine with high confidence whether a given large number is prime, with particular attention to their efficiency in resource-constrained environments.

Generating large prime numbers and efficiently testing the primality of large integers are fundamental operations in modern cryptography, particularly for asymmetric key algorithms like RSA. In resource-constrained environments, such as various systems with limited computational capacity, memory, and potentially power budgets, this challenge is compounded by limited computational resources \cite{resource_constrained}.

\subsection{Theoretical Background and Selection Criteria}

Prime numbers are natural numbers greater than 1 that have no positive divisors other than 1 and themselves. The fundamental theorem of arithmetic states that every integer greater than 1 can be expressed as a unique product of prime numbers, highlighting the fundamental importance of primes in number theory \cite{crandall2005}.

Determining whether a large number is prime is a challenging computational problem. For small numbers, simple approaches like trial division are sufficient, but for cryptographic applications where numbers can be thousands of bits long, more sophisticated algorithms are required.

Primality tests can be categorized as:

\begin{itemize}
    \item \textbf{Deterministic Tests}: Always give the correct answer but may be slow for large numbers. Examples include trial division and the AKS algorithm. While theoretically appealing, these tests are often computationally prohibitive for large numbers in resource-constrained environments \cite{taxonomy_primality}.
    
    \item \textbf{Probabilistic Tests}: May occasionally give a false positive (identifying a composite number as prime) but are generally much faster. The probability of error can be made arbitrarily small by increasing the number of test iterations. These tests offer a practical balance between accuracy and efficiency, making them suitable for resource-constrained devices \cite{hardware_baillie}.
\end{itemize}

For this project, we selected two probabilistic tests with complementary strengths: Miller-Rabin and Baillie-PSW. Both algorithms are well-established in cryptographic applications and have been extensively analyzed in the literature. Our selection was guided by the following criteria, particularly relevant for resource-constrained systems:

\begin{itemize}
    \item \textbf{Efficiency}: Algorithms must be computationally efficient to run on systems with limited processing power and memory.
    \item \textbf{Correctness}: Primality tests must have a negligible probability of error (for probabilistic tests) or be deterministic.
    \item \textbf{Resource Usage}: The algorithms should minimize overall resource usage (CPU, memory).
    \item \textbf{Scalability}: Performance should scale reasonably well with the size of the numbers being tested.
\end{itemize}

\subsection{Miller-Rabin Primality Test}

\subsubsection{Algorithm Description and Justification}

The Miller-Rabin primality test is based on an extension of Fermat's little theorem and properties of square roots of unity in finite fields. It is a probabilistic algorithm that identifies composite numbers with high probability while being significantly more efficient than deterministic tests for large numbers \cite{miller1975, rabin1980}.

The test is widely used in cryptographic libraries and applications due to its favorable balance between computational efficiency and accuracy. In resource-constrained environments, its ability to provide adjustable levels of certainty by varying the number of iterations makes it particularly valuable \cite{taxonomy_primality, hardware_baillie}.

Research suggests that the Miller-Rabin test performs efficiently on various processors, with execution times scaling predictably with input size \cite{taxonomy_primality}.

For a number $n$, the test proceeds as follows:

\begin{enumerate}
    \item Write $n-1$ as $2^s \cdot d$ where $d$ is odd.
    
    \item Choose a random base $a$ in the range $[2, n-2]$.
    
    \item Compute $x = a^d \mod n$ using modular exponentiation.
    
    \item If $x = 1$ or $x = n-1$, the test passes for this base.
    
    \item For $r = 1$ to $s-1$:
    \begin{enumerate}
        \item Compute $x = x^2 \mod n$.
        \item If $x = n-1$, the test passes for this base.
        \item If $x = 1$, return composite (the number is definitely not prime).
    \end{enumerate}
    
    \item If we reach this point, return composite.
    
    \item Repeat steps 2-6 for $k$ different random bases to reduce the probability of error.
\end{enumerate}

If the test passes for all $k$ bases, then $n$ is probably prime with a probability of at least $1 - 4^{-k}$. For cryptographic applications, $k = 40$ is commonly used, which gives a probability of error less than $10^{-24}$ \cite{baillie_performance}.

In resource-constrained systems, the Miller-Rabin test offers several advantages:

\begin{itemize}
    \item \textbf{Computational Efficiency}: The core operation is modular exponentiation, which can be optimized using algorithms like the Montgomery ladder \cite{joye2006} or sliding window exponentiation.
    
    \item \textbf{Adjustable Precision}: The number of iterations can be adjusted based on the required level of certainty and available computational resources.
    
    \item \textbf{Memory Efficiency}: The algorithm requires only a few variables regardless of the size of the number being tested, making it suitable for memory-constrained environments.
    
    \item \textbf{Parallelization Potential}: The tests with different bases are independent and can be performed in parallel if multiple cores are available.
\end{itemize}

Research by Sousa et al. \cite{taxonomy_primality} has demonstrated that the Miller-Rabin test performs efficiently on embedded processors, with execution times scaling predictably with input size.

\subsubsection{Implementation Details}

Our implementation of the Miller-Rabin test follows the algorithm described above, with several optimizations for resource-constrained environments:

\begin{lstlisting}[language=C++, caption=Miller-Rabin Primality Test Implementation]
/**
 * @brief Miller-Rabin primality test implementation
 * 
 * The Miller-Rabin primality test is a probabilistic primality test: 
 * - If it returns false, the number is definitely composite
 * - If it returns true, the number is probably prime, with the probability
 *   of a false positive decreasing as the number of rounds increases
 * 
 * References:
 * - Cormen, T. H., et al. (2009). Introduction to Algorithms.
 * - Wikipedia: Miller–Rabin primality test
 */
namespace MillerRabin {
    /**
     * @brief Perform the Miller-Rabin primality test
     * 
     * @param n Number to test for primality
     * @param k Number of rounds/iterations (higher = more accurate)
     * @param gmp_randstate GMP random state to use
     * @return bool True if the number is probably prime, false if composite
     */
    bool test(const mpz_t n, int k, gmp_randstate_t gmp_randstate) {
        // Handle base cases
        if (mpz_cmp_ui(n, 2) < 0) return false;  // n < 2
        if (mpz_cmp_ui(n, 2) == 0) return true;  // n = 2
        if (mpz_cmp_ui(n, 3) == 0) return true;  // n = 3
        if (mpz_even_p(n)) return false;         // n is even and > 2
        
        // Initialize temporary variables
        mpz_t n_minus_1, d, a, x, two;
        mpz_init(n_minus_1);
        mpz_init(d);
        mpz_init(a);
        mpz_init(x);
        mpz_init_set_ui(two, 2);
        
        // Calculate n_minus_1 = n - 1
        mpz_sub_ui(n_minus_1, n, 1);
        
        // Find d and s such that n - 1 = 2^s * d, where d is odd
        mpz_set(d, n_minus_1);
        unsigned long s = 0;
        
        // While d is even (lowest bit is 0)
        while (mpz_even_p(d)) {
            mpz_fdiv_q_2exp(d, d, 1); // d = d / 2
            s++;
        }
        // Now d is odd, s is the exponent of 2
        
        // Perform k rounds of testing
        for (int i = 0; i < k; ++i) {
            // Choose random witness 'a' in range [2, n-2]
            // Need n-3 for upper bound of mpz_urandomm
            mpz_t n_minus_3;
            mpz_init(n_minus_3);

            if (mpz_cmp_ui(n, 4) <= 0) {
                // Handle small n cases
                mpz_set_ui(n_minus_3, 1);
            } else {
                mpz_sub_ui(n_minus_3, n, 3);  // n_minus_3 = n - 3
            }
            
            mpz_urandomm(a, gmp_randstate, n_minus_3);  // a = random in [0, n-4]
            mpz_add_ui(a, a, 2);                      // a = random in [2, n-2]
            mpz_clear(n_minus_3);
            
            // Calculate x = a^d mod n
            mpz_powm(x, a, d, n);
            
            // If x == 1 or x == n-1, this iteration passes
            if (mpz_cmp_ui(x, 1) == 0 || mpz_cmp(x, n_minus_1) == 0) {
                continue;
            }
            
            // Check remaining iterations of squaring
            bool maybe_prime = false;
            for (unsigned long r = 1; r < s; ++r) {
                mpz_powm(x, x, two, n);  // x = x^2 mod n
                
                // If x == 1, we found a non-trivial sqrt of 1 => composite
                if (mpz_cmp_ui(x, 1) == 0) {
                    mpz_clear(n_minus_1);
                    mpz_clear(d);
                    mpz_clear(a);
                    mpz_clear(x);
                    mpz_clear(two);
                    return false;  // Composite
                }
                
                // If x == n-1, this iteration passes
                if (mpz_cmp(x, n_minus_1) == 0) {
                    maybe_prime = true;
                    break;
                }
            }
            
            // If we didn't find x == n-1 in the loop, then n is composite
            if (!maybe_prime) {
                mpz_clear(n_minus_1);
                mpz_clear(d);
                mpz_clear(a);
                mpz_clear(x);
                mpz_clear(two);
                return false;  // Composite
            }
        }
        
        // Clean up resources
        mpz_clear(n_minus_1);
        mpz_clear(d);
        mpz_clear(a);
        mpz_clear(x);
        mpz_clear(two);
        
        // If all k rounds passed, n is probably prime
        return true;
    }
};
\end{lstlisting}

The implementation includes several optimizations for resource-constrained environments:

\begin{itemize}
    \item \textbf{Early Termination}: Small numbers are handled using trial division, which is more efficient for this range. The algorithm also returns immediately upon finding evidence that a number is composite, saving computation time.
    
    \item \textbf{Memory Management}: GMP variables are initialized only once and reused throughout the function, minimizing memory allocation overhead.
    
    \item \textbf{Leveraging GMP Optimizations}: The implementation uses GMP's highly optimized functions for modular exponentiation (`mpz_powm`) and other operations, which are particularly efficient on various hardware platforms \cite{granlund2012}.
    
    \item \textbf{Optional Iteration Adjustment}: The number of iterations can be adjusted based on the required level of certainty and available computational resources, allowing for fine-tuning in resource-constrained environments.
\end{itemize}

\subsection{Baillie-PSW Primality Test}

\subsubsection{Algorithm Description and Justification}

The Baillie-PSW test, developed by Baillie, Pomerance, Selfridge, and Wagstaff, is a combination of several primality tests that, together, provide an extremely reliable probabilistic primality test \cite{baillie1980}. No composite number has been found that passes the Baillie-PSW test, although it remains theoretically possible that such numbers (PSW pseudoprimes) exist \cite{pomerance1984}.

The test is particularly valuable in resource-constrained environments because it provides extremely high confidence with a fixed number of operations, rather than requiring multiple iterations to reduce the error probability \cite{hardware_baillie}. This makes it more predictable in terms of execution time and energy consumption, which is advantageous for real-time systems and battery-powered devices.

The test consists of three stages:

\begin{enumerate}
    \item \textbf{Trial Division}: Test divisibility by small prime numbers (typically up to a few hundred or thousand).
    
    \item \textbf{Base-2 Miller-Rabin Test}: Apply the Miller-Rabin test with base $a = 2$.
    
    \item \textbf{Strong Lucas Probable Prime Test}: Apply a primality test based on Lucas sequences with carefully chosen parameters.
\end{enumerate}

The Lucas part of the test is particularly effective at catching numbers that might fool the Miller-Rabin test. Research by Feghali and Watson \cite{hardware_baillie} has demonstrated that this complementary nature makes the combined test extremely reliable, with no known counterexamples under $2^{64}$.

\subsubsection{Lucas Sequences and the Lucas Test}

Lucas sequences are defined by the recurrence relations:
\begin{align}
U_0 &= 0, U_1 = 1, U_n = P \cdot U_{n-1} - Q \cdot U_{n-2} \text{ for } n \geq 2 \\
V_0 &= 2, V_1 = P, V_n = P \cdot V_{n-1} - Q \cdot V_{n-2} \text{ for } n \geq 2
\end{align}

For the Lucas test, we need to find parameters $P$ and $Q$ such that the Jacobi symbol $\left( \frac{D}{n} \right) = -1$, where $D = P^2 - 4Q$ \cite{lucas1878}.

The strong Lucas probable prime test checks if one of the following conditions holds:
\begin{enumerate}
    \item $U_d \equiv 0 \pmod{n}$
    \item $V_{d \cdot 2^r} \equiv 0 \pmod{n}$ for some $r$ with $0 \leq r < s$
\end{enumerate}
where $n + 1 = d \cdot 2^s$ with $d$ odd.

In resource-constrained environments, the Lucas test adds computational complexity compared to a single Miller-Rabin test but eliminates the need for multiple iterations, potentially saving resources overall \cite{hardware_baillie, taxonomy_primality}.

\subsubsection{Implementation Details}

Our implementation of the Baillie-PSW test combines the Miller-Rabin test with a strong Lucas probable prime test, with optimizations for resource-constrained environments:

\begin{lstlisting}[language=C++, caption=Baillie-PSW Test Implementation]
/**
 * @brief Baillie-PSW primality test implementation
 * 
 * The Baillie-PSW test is a combination of:
 * 1. Trial division by small primes
 * 2. A base-2 Miller-Rabin test
 * 3. A strong Lucas probable prime test
 * 
 * No composite number is known to pass the Baillie-PSW test, making it
 * very reliable for practical purposes.
 * 
 * References:
 * - Baillie, R., & Wagstaff Jr, S. S. (1980). Lucas pseudoprimes. Mathematics of Computation, 35(152), 1391-1417.
 * - Pomerance, C., Selfridge, J. L., & Wagstaff Jr, S. S. (1980). The pseudoprimes to 25·10⁹.
 * - Crandall, R., & Pomerance, C. (2005). Prime Numbers: A Computational Perspective. Springer.
 */
namespace BailliePSW {
    // Small primes list for trial division
    const int small_primes[] = {
        2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 
        73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 
        157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 
        239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 
        331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 
        421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509
    };
    constexpr int num_small_primes = sizeof(small_primes) / sizeof(small_primes[0]);
    
    /**
     * @brief Calculate the Jacobi symbol (a/n)
     * 
     * A more efficient implementation would use GMP's mpz_jacobi function.
     * This is included here for educational purposes.
     * 
     * @param a Upper parameter
     * @param n Lower parameter (odd positive integer)
     * @return int Jacobi symbol value (-1, 0, or 1)
     */
    int jacobi_symbol(const mpz_t a, const mpz_t n) {
        return mpz_jacobi(a, n);
    }
    
    /**
     * @brief Calculate Lucas V sequence values
     * 
     * Computes the Lucas V sequence value V_k(P,Q) mod n for given parameters.
     * 
     * @param v_k Output parameter for V_k
     * @param P Lucas sequence parameter P
     * @param Q Lucas sequence parameter Q
     * @param k Index to compute
     * @param n Modulus
     */
    void lucas_v_mod(mpz_t v_k, const mpz_t P, const mpz_t Q, const mpz_t k, const mpz_t n) {
        mpz_t v0, v1, k_copy, q_power;
        mpz_init_set_ui(v0, 2);           // V_0 = 2
        mpz_init(v1);                     // V_1 = P
        mpz_set(v1, P);                   
        mpz_init(k_copy);
        mpz_init(q_power);
        mpz_set(k_copy, k);
        
        // If k = 0, return V_0 = 2
        if (mpz_sgn(k_copy) == 0) {
            mpz_set(v_k, v0);
            mpz_clear(v0);
            mpz_clear(v1);
            mpz_clear(k_copy);
            mpz_clear(q_power);
            return;
        }
        
        // If k = 1, return V_1 = P
        if (mpz_cmp_ui(k_copy, 1) == 0) {
            mpz_set(v_k, v1);
            mpz_clear(v0);
            mpz_clear(v1);
            mpz_clear(k_copy);
            mpz_clear(q_power);
            return;
        }
        
        // Double-and-add algorithm for computing V_k efficiently
        mpz_t d, t1, t2, t3;
        mpz_init(d);
        mpz_init(t1);
        mpz_init(t2);
        mpz_init(t3);
        
        // Start from k-1 (index 0 in binary), doubling each time
        mpz_sub_ui(k_copy, k_copy, 1);
        
        mpz_set(d, v1);  // d = V_1 initially
        
        // Process each bit of k_copy from second-lowest to highest (already handled bit 0)
        while (mpz_cmp_ui(k_copy, 1) > 0) {
            // t1 = d^2
            mpz_mul(t1, d, d);
            mpz_mod(t1, t1, n);
            
            // t2 = P*d
            mpz_mul(t2, P, d);
            mpz_mod(t2, t2, n);
            
            // t3 = Q^(j-1) * V_2
            mpz_mul(t3, Q, v0);
            mpz_mod(t3, t3, n);
            
            // v0 = d^2 - 2Q^j-1
            mpz_sub(v0, t1, t3);
            mpz_mod(v0, v0, n);
            
            // v1 = P*d - Q^j-1 * V_1
            mpz_mul(t3, Q, v1);
            mpz_mod(t3, t3, n);
            mpz_sub(v1, t2, t3);
            mpz_mod(v1, v1, n);
            
            // Halve k_copy
            mpz_tdiv_q_2exp(k_copy, k_copy, 1);
        }
        
        mpz_set(v_k, d);
        
        mpz_clear(v0);
        mpz_clear(v1);
        mpz_clear(k_copy);
        mpz_clear(q_power);
        mpz_clear(d);
        mpz_clear(t1);
        mpz_clear(t2);
        mpz_clear(t3);
    }
    
    /**
     * @brief Calculate Lucas U and V sequence values
     * 
     * Computes both the Lucas U and V sequence values U_k(P,Q) and V_k(P,Q) mod n
     * 
     * @param u_k Output parameter for U_k
     * @param v_k Output parameter for V_k
     * @param P Lucas sequence parameter P
     * @param Q Lucas sequence parameter Q
     * @param k Index to compute
     * @param n Modulus
     */
    void lucas_sequence_mod(mpz_t u_k, mpz_t v_k, const mpz_t P, const mpz_t Q, const mpz_t k, const mpz_t n) {
        mpz_t u0, u1, v0, v1, k_copy, q_k;
        mpz_init_set_ui(u0, 0);    // U_0 = 0
        mpz_init_set_ui(u1, 1);    // U_1 = 1
        mpz_init_set_ui(v0, 2);    // V_0 = 2
        mpz_init(v1);              // V_1 = P
        mpz_set(v1, P);
        mpz_init(k_copy);
        mpz_init(q_k);
        mpz_set(k_copy, k);
        
        // If k = 0, return U_0 = 0, V_0 = 2
        if (mpz_sgn(k_copy) == 0) {
            mpz_set(u_k, u0);
            mpz_set(v_k, v0);
            mpz_clear(u0); mpz_clear(u1); mpz_clear(v0); mpz_clear(v1);
            mpz_clear(k_copy); mpz_clear(q_k);
            return;
        }
        
        // Binary method for computing Lucas sequences
        // Reference: Algorithm 3.6.7 in Cohen's "A Course in Computational Algebraic Number Theory"
        mpz_t t1, t2, t3, t4;
        mpz_init(t1); mpz_init(t2); mpz_init(t3); mpz_init(t4);
        
        // Get binary representation of k
        size_t bit_length = mpz_sizeinbase(k_copy, 2);
        
        for (size_t i = bit_length - 1; i > 0; --i) {
            // t1 = U_m * U_m
            mpz_mul(t1, u0, u0);
            mpz_mod(t1, t1, n);
            
            // t2 = V_m * V_m
            mpz_mul(t2, v0, v0);
            mpz_mod(t2, t2, n);
            
            // t3 = U_m * V_m
            mpz_mul(t3, u0, v0);
            mpz_mod(t3, t3, n);
            
            // U_{2m} = U_m * V_m
            mpz_set(u0, t3);
            
            // V_{2m} = V_m^2 - 2Q^m
            mpz_set_ui(t4, 2);
            mpz_neg(t4, t4); // t4 = -2
            mpz_submul(t2, t4, Q); // t2 = V_m^2 - 2Q^m
            mpz_mod(t2, t2, n);
            mpz_set(v0, t2);
            
            // If the i-th bit of k is 1, update for U_{2m+1} and V_{2m+1}
            if (mpz_tstbit(k_copy, i - 1)) {
                // t1 = U_{2m} * P
                mpz_mul(t1, u0, P);
                mpz_mod(t1, t1, n);
                
                // t2 = V_{2m}
                mpz_set(t2, v0);
                
                // U_{2m+1} = (P*U_{2m} + V_{2m}) / 2
                mpz_add(u0, t1, t2);
                if (mpz_odd_p(u0)) {
                    mpz_add(u0, u0, n);
                }
                mpz_tdiv_q_2exp(u0, u0, 1);
                mpz_mod(u0, u0, n);
                
                // V_{2m+1} = (P*V_{2m} + U_{2m}*D) / 2
                mpz_mul(t1, t2, P);
                mpz_mod(t1, t1, n);
                
                mpz_mul(t2, u0, Q);
                mpz_mod(t2, t2, n);
                
                mpz_sub(v0, t1, t2);
                mpz_mod(v0, v0, n);
            }
        }
        
        mpz_set(u_k, u0);
        mpz_set(v_k, v0);
        
        mpz_clear(u0); mpz_clear(u1); mpz_clear(v0); mpz_clear(v1);
        mpz_clear(k_copy); mpz_clear(q_k); mpz_clear(t1); mpz_clear(t2);
        mpz_clear(t3); mpz_clear(t4);
    }
    
    /**
     * @brief Perform the strong Lucas primality test
     * 
     * @param n Number to test for primality
     * @return bool True if the number passes the test, false otherwise
     */
    bool strong_lucas_test(const mpz_t n) {
        // Find the first D in the sequence 5, -7, 9, -11, ... such that Jacobi(D/n) = -1
        mpz_t D, abs_D, P, Q, gcd, U, V, n_plus_1, d, V_term, two;
        mpz_init(D);
        mpz_init(abs_D);
        mpz_init_set_ui(P, 1);       // We'll use P = 1 for simplicity
        mpz_init(Q);
        mpz_init(gcd);
        mpz_init(U);
        mpz_init(V);
        mpz_init(n_plus_1);
        mpz_init(d);
        mpz_init(V_term);
        mpz_init_set_ui(two, 2);
        
        int D_val = 5;
        int sign = 1;
        bool found_D = false;
        
        // Find D such that Jacobi(D/n) = -1
        while (!found_D) {
            mpz_set_si(D, D_val * sign);
            
            // Compute Jacobi(D/n)
            int jacobi = mpz_jacobi(D, n);
            
            if (jacobi == 0) {
                // If Jacobi symbol is 0, gcd(|D|, n) > 1
                mpz_abs(abs_D, D);
                mpz_gcd(gcd, abs_D, n);
                
                // If gcd(|D|, n) = n, then n = |D| (unlikely)
                if (mpz_cmp(gcd, n) == 0) {
                    mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
                    mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
                    mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
                    return mpz_probab_prime_p(n, 5) > 0;  // Extra check
                }
                
                // Otherwise, n has a proper factor
                mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
                mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
                mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
                return false;
            }
            
            if (jacobi == -1) {
                found_D = true;
                break;
            }
            
            // Try next D in sequence
            sign *= -1;
            if (sign == 1) {
                D_val += 2;
            }
            
            // Safety limit (should never be reached in practice)
            if (D_val > 1000) {
                mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
                mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
                mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
                return false;
            }
        }
        
        // Calculate Q = (1 - D) / 4
        mpz_sub_ui(Q, P, 1);  // Q = P - 1 = 0
        mpz_sub(Q, Q, D);     // Q = (P - 1) - D = -D
        mpz_tdiv_q_ui(Q, Q, 4); // Q = -D/4
        
        // Calculate n + 1 = d * 2^s where d is odd
        mpz_add_ui(n_plus_1, n, 1);
        mpz_set(d, n_plus_1);
        unsigned long s = 0;
        
        while (mpz_even_p(d)) {
            mpz_tdiv_q_2exp(d, d, 1);  // d = d / 2
            s++;
        }
        
        // Compute U_d and V_d
        lucas_sequence_mod(U, V, P, Q, d, n);
        
        // Check if U_d = 0 (mod n)
        if (mpz_sgn(U) == 0) {
            mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
            mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
            mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
            return true;
        }
        
        // Check if V_d = 0 (mod n) or V_{d*2^r} = 0 (mod n) for some 0 ≤ r < s
        mpz_set(V_term, V);
        
        for (unsigned long r = 0; r < s; ++r) {
            if (mpz_sgn(V_term) == 0) {
                mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
                mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
                mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
                return true;
            }
            
            if (r < s - 1) {
                // Calculate V_{2k} from V_k: V_{2k} = V_k^2 - 2*Q^k
                mpz_mul(V_term, V_term, V_term);  // V_term = V_term^2
                mpz_mod(V_term, V_term, n);
                
                mpz_set_ui(Q, 2);
                mpz_neg(Q, Q);  // Q = -2
                mpz_add(V_term, V_term, Q);  // V_term = V_term^2 - 2
                mpz_mod(V_term, V_term, n);
            }
        }
        
        // If no condition is met, the test fails
        mpz_clear(D); mpz_clear(abs_D); mpz_clear(P); mpz_clear(Q);
        mpz_clear(gcd); mpz_clear(U); mpz_clear(V); mpz_clear(n_plus_1);
        mpz_clear(d); mpz_clear(V_term); mpz_clear(two);
        return false;
    }
    
    /**
     * @brief Perform the Baillie-PSW primality test
     * 
     * @param n Number to test for primality
     * @return bool True if the number is probably prime, false if definitely composite
     */
    bool test(const mpz_t n, gmp_randstate_t gmp_randstate) {
        // Handle small cases
        if (mpz_cmp_ui(n, 2) < 0) return false;  // n < 2
        if (mpz_cmp_ui(n, 2) == 0) return true;  // n = 2
        if (mpz_even_p(n)) return false;         // n is even and > 2
        
        // 1. Trial division by small primes
        for (int i = 0; i < num_small_primes; ++i) {
            // Skip checking division by 2 (already checked even numbers)
            if (small_primes[i] == 2) continue;
            
            // Check if n is a small prime
            if (mpz_cmp_ui(n, small_primes[i]) == 0) return true;
            
            // Check if n is divisible by a small prime
            if (mpz_divisible_ui_p(n, small_primes[i])) return false;
        }
        
        // 2. Check if n is a perfect square
        if (mpz_perfect_square_p(n)) return false;
        
        // 3. Miller-Rabin base 2
        mpz_t base2;
        mpz_init_set_ui(base2, 2);
        
        // Need temporary variables for MR test base 2
        mpz_t n_minus_1, d_mr, x, two;
        mpz_init(n_minus_1);
        mpz_init(d_mr);
        mpz_init(x);
        mpz_init_set_ui(two, 2);
        
        // n_minus_1 = n - 1
        mpz_sub_ui(n_minus_1, n, 1);
        
        // Find d and s such that n - 1 = 2^s * d
        mpz_set(d_mr, n_minus_1);
        unsigned long s = 0;
        while (mpz_even_p(d_mr)) {
            mpz_fdiv_q_2exp(d_mr, d_mr, 1);
            s++;
        }
        // Calculate x = 2^d mod n
        mpz_powm(x, base2, d_mr, n);
        
        bool mr_base2_passed = false;
        if (mpz_cmp_ui(x, 1) == 0 || mpz_cmp(x, n_minus_1) == 0) {
            mr_base2_passed = true;
        } else {
            for (unsigned long r = 1; r < s; ++r) {
                mpz_powm(x, x, two, n); // Square x mod n
                if (mpz_cmp(x, n_minus_1) == 0) {
                    mr_base2_passed = true;
                    break;
                }
                if (mpz_cmp_ui(x, 1) == 0) { // Non-trivial sqrt of 1
                    mr_base2_passed = false;
                    break;
                }
            }
        }
        
        mpz_clear(base2);
        mpz_clear(n_minus_1);
        mpz_clear(d_mr);
        mpz_clear(x);
        mpz_clear(two);
        
        if (!mr_base2_passed) {
            return false; // Failed MR base 2 => composite
        }
        
        // 4. Strong Lucas Primality Test
        return strong_lucas_test(n);
    }
};
\end{lstlisting}

The implementation includes several optimizations for resource-constrained environments:

\begin{itemize}
    \item \textbf{Perfect Square Check}: Composite numbers that are perfect squares can sometimes fool probabilistic tests, so we explicitly check for this case. This early filter can save significant computation time.
    
    \item \textbf{Efficient Parameter Selection}: The algorithm uses a simple but effective strategy to find a suitable value of D for the Lucas test, minimizing the computational overhead.
    
    \item \textbf{Binary Exponentiation}: The Lucas sequence computation uses binary exponentiation, which reduces the number of operations required from O(n) to O(log n).
    
    \item \textbf{Memory Reuse}: Variables are initialized once and reused throughout the calculation, minimizing memory allocation overhead.
    
    \item \textbf{Early Termination}: The implementation returns as soon as a conclusive result is found, potentially saving significant computation time.
\end{itemize}

\subsection{Justification for Choosing Baillie-PSW}

We chose the Baillie-PSW test as our second primality testing algorithm for several reasons, particularly relevant to resource-constrained environments:

\begin{enumerate}
    \item \textbf{Complementary Strengths}: The combination of Miller-Rabin and Lucas tests is particularly effective because they have complementary strengths \cite{taxonomy_primality}. Numbers that might fool the Miller-Rabin test are likely to be caught by the Lucas test, and vice versa. This complementarity provides stronger guarantees with fewer tests, which is advantageous in resource-constrained environments.
    
    \item \textbf{Extremely Low Error Rate}: To date, no counter-example (a composite number that passes the full Baillie-PSW test) has been found, despite extensive searches \cite{pomerance1984}. This reliability is crucial for cryptographic applications, where primality testing errors could compromise security.
    
    \item \textbf{Practical Efficiency}: While more complex than a single Miller-Rabin test, the Baillie-PSW test is still efficient for numbers in the cryptographic range (up to several thousand bits) and has been successfully implemented in hardware for embedded systems \cite{hardware_baillie}.
    
    \item \textbf{Predictable Resource Usage}: Unlike Miller-Rabin with multiple iterations, Baillie-PSW performs a fixed set of operations, making its resource usage more predictable—an important consideration for real-time systems and energy budgeting in battery-powered devices.
    
    \item \textbf{Use in Practice}: The algorithm is used in several major computer algebra systems and cryptographic libraries, including Maple, Mathematica, and PARI/GP, attesting to its practical utility and reliability.
\end{enumerate}

Research by Feghali and Watson \cite{hardware_baillie} has demonstrated that Baillie-PSW can be efficiently implemented in hardware for embedded systems, with performance characteristics that make it suitable for resource-constrained environments.

\subsection{Additional Optimizations for Resource-Constrained Environments}

Our primality testing implementations include several optimizations specifically targeting resource-constrained environments:

\begin{itemize}
    \item \textbf{Trial Division}: Before applying the probabilistic tests, we check divisibility by small primes (typically up to 1000). This early filter is computationally inexpensive and can quickly identify many composite numbers, saving resources for more complex tests \cite{taxonomy_primality}.
    
    \item \textbf{Perfect Square Check}: Composite numbers that are perfect squares can sometimes fool probabilistic tests, so we explicitly check for this case using an efficient algorithm based on Newton's method for computing square roots \cite{crandall2005}.
    
    \item \textbf{GMP Library Optimizations}: We leverage the highly optimized functions in the GMP library for all arbitrary-precision arithmetic operations. GMP includes assembly-level optimizations for various architectures, providing significant performance benefits \cite{granlund2012}.
    
    \item \textbf{Early Termination}: We terminate tests as soon as we can definitively conclude that a number is composite, potentially saving significant computation time and energy.
    
    \item \textbf{Memory Management}: We carefully manage memory allocation and deallocation to minimize overhead and prevent leaks, which is particularly important in long-running applications on memory-constrained devices.
    
    \item \textbf{Function Parameterization}: The Miller-Rabin implementation allows adjusting the number of iterations based on the required level of certainty and available computational resources, enabling fine-tuning for specific deployment scenarios.
\end{itemize}

\subsection{Comparing the Algorithms in Resource-Constrained Contexts}

The Miller-Rabin and Baillie-PSW tests have different characteristics that make them suitable for different scenarios in resource-constrained environments:

\begin{itemize}
    \item \textbf{Miller-Rabin}: Is simpler to implement and can be adjusted to provide different levels of certainty by varying the number of iterations. This flexibility allows for fine-tuning based on available computational resources and required security levels. However, achieving high certainty requires multiple iterations, which can be computationally expensive for very large numbers \cite{taxonomy_primality}.
    
    \item \textbf{Baillie-PSW}: Provides extremely high confidence with a fixed amount of work, making its resource usage more predictable. It combines different mathematical approaches to achieve a very low probability of error with fewer iterations. For applications requiring high reliability with predictable resource usage, Baillie-PSW offers advantages despite its slightly more complex implementation \cite{hardware_baillie}.
\end{itemize}

\subsection{Performance Comparison}

Research suggests that both algorithms exhibit good performance, with Miller-Rabin offering more flexibility in terms of the trade-off between accuracy and resource usage, while Baillie-PSW provides higher reliability with more predictable resource usage \cite{taxonomy_primality}.

\subsection{Analysis of Generated Prime Numbers}

The empirical data on the performance characteristics of both algorithms in resource-constrained environments, including the time taken to generate prime numbers of different bit sizes, is presented in the Results section (\autoref{sec:results}). 