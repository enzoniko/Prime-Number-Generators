\section{Results and Evaluation}

This section presents the experimental results of our performance evaluation of random number generation and primality testing algorithms in resource-constrained environments. Our analysis follows established benchmarking practices for embedded systems \cite{embedded_benchmarking} while adapting them to the specific requirements of cryptographic primitives.

\subsection{Random Number Generation Algorithms}

\subsubsection{Execution Time}
Figure \ref{fig:rng_execution_time} shows the execution time of LCG and Xoshiro256++ across different system configurations.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/rng_execution_time.png}
\caption{Execution time comparison of random number generation algorithms}
\label{fig:rng_execution_time}
\end{figure}

The results demonstrate that the LCG algorithm consistently outperforms Xoshiro256++ in terms of execution speed, with an average improvement of 62\% across all tested configurations. This performance advantage is particularly pronounced in the most resource-constrained environments, where the LCG algorithm's simpler computational requirements translate to significant time savings \cite{energy_efficient}.

\subsubsection{Memory Usage}
Figure \ref{fig:rng_memory_usage} illustrates the memory footprint of each algorithm during operation.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/rng_memory_usage.png}
\caption{Memory usage comparison of random number generation algorithms}
\label{fig:rng_memory_usage}
\end{figure}

The memory usage analysis reveals that LCG requires only 4 bytes of state, while Xoshiro256++ requires 32 bytes (256 bits). This 8x difference in memory footprint is particularly significant for environments with severe memory constraints, such as low-end IoT devices \cite{iot_survey}, where every byte of RAM is valuable.

\subsubsection{Energy Consumption}
Figure \ref{fig:rng_energy} presents the energy consumption measurements for both algorithms.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/rng_energy.png}
\caption{Energy consumption comparison of random number generation algorithms}
\label{fig:rng_energy}
\end{figure}

The energy consumption patterns closely follow the execution time trends, with LCG consuming approximately 58\% less energy than Xoshiro256++ across all configurations. This correlation is expected, as processor active time is the primary driver of energy consumption in these algorithms \cite{energy_efficient}. For battery-powered devices, this energy efficiency translates directly to extended operational lifespan.

\subsubsection{Statistical Quality}
Table \ref{tab:rng_statistical_quality} presents the results of statistical quality tests for both algorithms.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Statistical Test} & \textbf{LCG} & \textbf{Xoshiro256++} \\
\hline
Frequency & Pass & Pass \\
Block Frequency & Partial Pass & Pass \\
Runs & Partial Pass & Pass \\
Longest Run of Ones & Pass & Pass \\
Rank & Fail & Pass \\
DFT (Spectral) & Fail & Pass \\
Non-overlapping Template & Partial Pass & Pass \\
Overlapping Template & Pass & Pass \\
Universal & Fail & Pass \\
Linear Complexity & Pass & Pass \\
Serial & Partial Pass & Pass \\
Approximate Entropy & Partial Pass & Pass \\
Cumulative Sums & Pass & Pass \\
Random Excursions & Partial Pass & Pass \\
Random Excursions Variant & Partial Pass & Pass \\
\hline
\end{tabular}
\caption{Statistical quality comparison based on NIST Statistical Test Suite \cite{nist_test_suite}}
\label{tab:rng_statistical_quality}
\end{table}

The statistical quality analysis, based on the NIST Statistical Test Suite \cite{nist_test_suite}, demonstrates a clear quality advantage for Xoshiro256++, which passes all statistical tests. In contrast, LCG fails or only partially passes several critical tests, highlighting the fundamental trade-off between performance and quality in random number generation algorithms.

\subsection{Primality Testing Algorithms}

\subsubsection{Execution Time}
Figure \ref{fig:primality_execution_time} compares the execution time of Miller-Rabin and Baillie-PSW tests across different input sizes.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/primality_execution_time.png}
\caption{Execution time comparison of primality testing algorithms}
\label{fig:primality_execution_time}
\end{figure}

The results show that the Miller-Rabin test with 4 rounds is consistently faster than the Baillie-PSW test, with the performance gap widening as input size increases. For 2048-bit numbers, Miller-Rabin is approximately 35\% faster. However, this advantage must be weighed against the probabilistic nature of the algorithm \cite{resource_constrained}.

\subsubsection{Memory Usage}
Figure \ref{fig:primality_memory_usage} shows the peak memory usage during primality testing.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/primality_memory_usage.png}
\caption{Memory usage comparison of primality testing algorithms}
\label{fig:primality_memory_usage}
\end{figure}

The Baillie-PSW test consumes approximately 22\% more memory than Miller-Rabin across all input sizes, primarily due to the additional state required for the Lucas test component. This increased memory requirement can be significant in severely memory-constrained environments \cite{iot_survey}.

\subsubsection{Energy Consumption}
Figure \ref{fig:primality_energy} illustrates the energy consumption of both algorithms.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/primality_energy.png}
\caption{Energy consumption comparison of primality testing algorithms}
\label{fig:primality_energy}
\end{figure}

The energy consumption analysis reveals that Miller-Rabin consumes approximately 30\% less energy than Baillie-PSW for equivalent operations. This energy efficiency advantage makes Miller-Rabin particularly suitable for battery-powered devices where energy conservation is critical \cite{energy_efficient}.

\subsubsection{Accuracy and Reliability}
Table \ref{tab:primality_accuracy} compares the accuracy of both primality testing algorithms.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Test Case} & \textbf{Miller-Rabin (4 rounds)} & \textbf{Baillie-PSW} \\
\hline
True Primes Identified & 100\% & 100\% \\
Carmichael Numbers Rejected & 99.99\% & 100\% \\
Strong Pseudoprimes Rejected & 99.99\% & 100\% \\
Theoretical False Positive Rate & $4^{-4}$ & No known counterexamples \\
\hline
\end{tabular}
\caption{Accuracy comparison of primality testing algorithms}
\label{tab:primality_accuracy}
\end{table}

While Miller-Rabin with 4 rounds provides a theoretical false positive rate of $4^{-4}$ (approximately 1 in 65,536), the Baillie-PSW test has no known counterexamples for numbers below $2^{64}$ when properly implemented. This superior accuracy makes Baillie-PSW the preferred choice for applications requiring high reliability, despite its performance disadvantages.

\subsection{Performance-Resource Trade-offs}

\subsubsection{Scaling with Resource Constraints}
Figure \ref{fig:scaling_constraints} illustrates how algorithm performance scales with increasingly severe resource constraints.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/scaling_constraints.png}
\caption{Algorithm performance scaling with resource constraints}
\label{fig:scaling_constraints}
\end{figure}

The data demonstrates that simpler algorithms (LCG and Miller-Rabin) degrade more gracefully under severe resource constraints compared to their more complex counterparts \cite{embedded_benchmarking}. This resilience to resource limitation is a critical factor when designing for worst-case scenarios in embedded environments.

\subsubsection{Application-Specific Considerations}
Table \ref{tab:application_considerations} provides recommendations for algorithm selection based on application requirements.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Application Type} & \textbf{RNG Recommendation} & \textbf{Primality Test} & \textbf{Key Factors} \\
\hline
Critical Security Applications & Xoshiro256++ & Baillie-PSW & Quality, Reliability \\
Battery-Powered IoT Devices & LCG & Miller-Rabin (2 rounds) & Energy, Speed \\
Real-time Systems & LCG & Miller-Rabin (1 round) & Deterministic Timing \\
Balanced Applications & Xoshiro256++ & Miller-Rabin (4 rounds) & Quality-Performance Balance \\
\hline
\end{tabular}
\caption{Algorithm recommendations based on application requirements}
\label{tab:application_considerations}
\end{table}

These recommendations highlight the importance of matching algorithm selection to specific application requirements, particularly in resource-constrained environments where trade-offs between performance, resource usage, and quality are unavoidable \cite{iot_survey, energy_efficient}.

\subsection{Discussion and Analysis}

\subsubsection{Performance-Quality Trade-offs}
Our results demonstrate the fundamental trade-off between performance and quality in both random number generation and primality testing. While simpler algorithms like LCG and Miller-Rabin offer better performance and lower resource usage, they provide lower quality or certainty. Conversely, more complex algorithms like Xoshiro256++ and Baillie-PSW offer superior quality at the cost of increased resource consumption.

This trade-off is particularly significant in resource-constrained environments, where the cost of additional computation or memory usage is high relative to available resources \cite{resource_constrained}. The selection of appropriate algorithms must therefore carefully balance these competing factors based on application-specific requirements and constraints.

\subsubsection{Implications for Cryptographic Applications}
For cryptographic applications in resource-constrained environments, our findings suggest that a hybrid approach may be optimal. For example:

\begin{itemize}
    \item Using LCG for non-critical random number generation (e.g., initialization vectors) while reserving Xoshiro256++ for key generation
    \item Implementing a tiered primality testing approach that begins with low-round Miller-Rabin tests and escalates to Baillie-PSW only for values that pass initial screening
    \item Dynamically adjusting algorithm parameters based on available resources and security requirements \cite{energy_efficient}
\end{itemize}

\subsubsection{Future Optimizations}
Based on our analysis, several optimization directions warrant further investigation:

\begin{itemize}
    \item Hardware-accelerated implementations of key operations (e.g., modular exponentiation) to improve performance without increasing resource consumption
    \item Adaptive algorithms that adjust their parameters based on available resources and required quality levels
    \item Specialized implementations that exploit specific characteristics of target platforms \cite{embedded_benchmarking}
    \item Energy-aware scheduling of cryptographic operations to minimize peak power consumption \cite{energy_efficient}
\end{itemize}

\subsection{Summary of Findings}
Our comprehensive evaluation of random number generation and primality testing algorithms in resource-constrained environments has yielded several key findings:

\begin{enumerate}
    \item LCG offers significantly better performance and lower resource usage than Xoshiro256++, but at the cost of reduced statistical quality
    \item Miller-Rabin with 4 rounds provides a good balance of performance and accuracy for most applications, while Baillie-PSW offers superior reliability at the cost of increased resource consumption
    \item Algorithm selection should be tailored to specific application requirements, with particular attention to the relative importance of performance, resource usage, and quality
    \item The performance gap between simpler and more complex algorithms widens under severe resource constraints, making algorithm selection increasingly critical in highly constrained environments \cite{iot_survey}
\end{enumerate}

These findings provide a foundation for informed algorithm selection in resource-constrained cryptographic applications, enabling developers to make optimal trade-offs based on their specific requirements and constraints. 